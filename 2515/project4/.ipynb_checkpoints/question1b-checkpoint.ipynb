{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'util.py' was not found in history, as a file, url, nor in the user namespace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mfind_user_code\u001b[0;34m(self, target, raw, py_only, skip_encoding_cookie, search_ns)\u001b[0m\n\u001b[1;32m   3644\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m                                              \u001b[0;31m# User namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3645\u001b[0;31m             \u001b[0mcodeobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3646\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'util' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-55e2507b39a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'util.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</Users/jianhuili/opt/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-46>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, arg_s)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/code.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0msearch_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'n'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_user_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_ns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m's'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mfind_user_code\u001b[0;34m(self, target, raw, py_only, skip_encoding_cookie, search_ns)\u001b[0m\n\u001b[1;32m   3646\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3647\u001b[0m             raise ValueError((\"'%s' was not found in history, as a file, url, \"\n\u001b[0;32m-> 3648\u001b[0;31m                                 \"nor in the user namespace.\") % target)\n\u001b[0m\u001b[1;32m   3649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodeobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'util.py' was not found in history, as a file, url, nor in the user namespace."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "import time\n",
    "import util\n",
    "\n",
    "from scipy.special import gammaln\n",
    "\n",
    "\n",
    "IMAGE_DIM = 28\n",
    "\n",
    "TRAIN_IMAGES_FILE = 'train-images-idx3-ubyte'\n",
    "TRAIN_LABELS_FILE = 'train-labels-idx1-ubyte'\n",
    "TEST_IMAGES_FILE = 't10k-images-idx3-ubyte'\n",
    "TEST_LABELS_FILE = 't10k-labels-idx1-ubyte'\n",
    "\n",
    "\n",
    "def beta_log_pdf(theta, a, b):\n",
    "    \"\"\"Log PDF of the beta distribution. We don't need this function, but we\n",
    "    include it in case you're interested. You need SciPy in order to use it.\"\"\"\n",
    "    norm_const = gammaln(a + b) - gammaln(a) - gammaln(b)\n",
    "    return norm_const + (a - 1.) * np.log(theta) + (b - 1.) * np.log(1. - theta)\n",
    "\n",
    "def beta_log_pdf_unnorm(theta, a, b):\n",
    "    \"\"\"Unnormalized log PDF of the beta distribution.\"\"\"\n",
    "    return (a - 1.) * np.log(theta) + (b - 1.) * np.log(1. - theta)\n",
    "\n",
    "def dirichlet_log_pdf(pi, a):\n",
    "    \"\"\"Log PDF of the Dirichlet distribution. We don't need this function, but we\n",
    "    include it in case you're interested. You need SciPy in order to use it.\"\"\"\n",
    "    norm_const = gammaln(a.sum()) - gammaln(a).sum()\n",
    "    return norm_const + np.sum((a - 1.) * np.log(pi))\n",
    "\n",
    "def dirichlet_log_pdf_unnorm(pi, a):\n",
    "    \"\"\"Unnormalized log PDF of the Dirichlet distribution.\"\"\"\n",
    "    return np.sum((a - 1.) * np.log(pi))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Params(object):\n",
    "    \"\"\"A class which represents the trainable parameters of the mixture model.\n",
    "        - pi: the mixing proportions, represented as a K-dimensional array. It must be a\n",
    "            probability distribution, i.e. the entries must be nonnegative and sum to 1.\n",
    "        - theta: The Bernoulli parameters for each pixel in each mixture component. This is\n",
    "            a K x D matrix, where rows correspond to mixture components and columns correspond\n",
    "            to pixels. \"\"\"\n",
    "    \n",
    "    def __init__(self, pi, theta):\n",
    "        self.pi = pi\n",
    "        self.theta = theta\n",
    "\n",
    "    @classmethod\n",
    "    def random_initialization(cls, num_components, num_pixels):\n",
    "        init_pi = np.ones(num_components) / num_components\n",
    "        init_theta = np.random.uniform(0.49, 0.51, size=(num_components, num_pixels))\n",
    "        return Params(init_pi, init_theta)\n",
    "\n",
    "class Prior(object):\n",
    "    \"\"\"A class representing the priors over parameters in the mixture model.\n",
    "        - a_mix: A scalar valued parameter for the Dirichlet prior over mixing proportions.\n",
    "        - a_pixels and b_pixels: The scalar-valued parameters for the beta prior over the entries of\n",
    "            theta. I.e., the entries of theta are assumed to be drawn i.i.d. from the distribution\n",
    "            Beta(a_pixels, b_pixels). \"\"\"\n",
    "    \n",
    "    def __init__(self, a_mix, a_pixels, b_pixels):\n",
    "        self.a_mix = a_mix\n",
    "        self.a_pixels = a_pixels\n",
    "        self.b_pixels = b_pixels\n",
    "\n",
    "    @classmethod\n",
    "    def default_prior(cls):\n",
    "        \"\"\"Return a Prior instance which has reasonable values.\"\"\"\n",
    "        return cls(2., 2., 2.)\n",
    "\n",
    "    @classmethod\n",
    "    def uniform_prior(cls):\n",
    "        \"\"\"Return a set of prior parameters which corresponds to a uniform distribution. Then\n",
    "        MAP estimation is equivalent to maximum likelihood.\"\"\"\n",
    "        return cls(1., 1., 1.)\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"A class implementing the mixture of Bernoullis model. The fields are:\n",
    "        - prior: an Prior instance\n",
    "        - params: a Params instance\"\"\"\n",
    "    \n",
    "    def __init__(self, prior, params):\n",
    "        self.prior = prior\n",
    "        self.params = params\n",
    "\n",
    "    @classmethod\n",
    "    def random_initialization(cls, prior, num_components, num_pixels):\n",
    "        params = Params.random_initialization(num_components, num_pixels)\n",
    "        return cls(prior, params)\n",
    "\n",
    "    def expected_joint_log_probability(self, X, R):\n",
    "        \"\"\"Compute the expected joint log probability, where the expectation is with respect to\n",
    "        the responsibilities R. This is the objective function being maximized in the M-step.\n",
    "        It's useful for verifying the optimality conditions in the M-step.\"\"\"\n",
    "        \n",
    "        total = 0.\n",
    "\n",
    "        # Prior over mixing proportions\n",
    "        total += dirichlet_log_pdf_unnorm(self.params.pi, self.prior.a_mix)\n",
    "\n",
    "        # Prior over pixel probabilities\n",
    "        total += np.sum(beta_log_pdf_unnorm(self.params.theta, self.prior.a_pixels, self.prior.b_pixels))\n",
    "\n",
    "        # Probability of assignments\n",
    "        total += np.sum(R * np.log(self.params.pi))\n",
    "        \n",
    "        # Matrix of log probabilities of observations conditioned on z\n",
    "        # The (i, k) entry is p(x^(i) | z^(i) = k)\n",
    "        log_p_x_given_z = np.dot(X, np.log(self.params.theta).T) + \\\n",
    "                          np.dot(1. - X, np.log(1. - self.params.theta).T)\n",
    "\n",
    "        # Observation probabilities\n",
    "        total += np.sum(R * log_p_x_given_z)\n",
    "\n",
    "        return total\n",
    "\n",
    "    def log_likelihood(self, X):\n",
    "        \"\"\"Compute the log-likelihood of the observed data, i.e. the log probability with the\n",
    "        latent variables marginalized out.\"\"\"\n",
    "        \n",
    "        # Matrix of log probabilities of observations conditioned on z\n",
    "        # The (i, k) entry is p(x^(i) | z^(i) = k)\n",
    "        log_p_x_given_z = np.dot(X, np.log(self.params.theta).T) + \\\n",
    "                          np.dot(1. - X, np.log(1. - self.params.theta).T)\n",
    "        log_p_z_x = log_p_x_given_z + np.log(self.params.pi)\n",
    "\n",
    "        # This is a numerically stable way to compute np.log(np.sum(np.exp(log_p_z_x), axis=1))\n",
    "        log_p_x = np.logaddexp.reduce(log_p_z_x, axis=1)\n",
    "\n",
    "        return log_p_x.sum()\n",
    "\n",
    "    def update_pi(self, R):\n",
    "        \"\"\"Compute the update for the mixing proportions in the M-step of the E-M algorithm.\n",
    "        You should derive the optimal value of pi (the one which maximizes the expected log\n",
    "        probability) by setting the partial derivatives of the Lagrangian to zero. You should\n",
    "        implement this in terms of NumPy matrix and vector operations, rather than a for loop.\"\"\"\n",
    "\n",
    "        ######################## Your code here #########################\n",
    "        pass\n",
    "\n",
    "\n",
    "        #################################################################\n",
    "        \n",
    "    def update_theta(self, X, R):\n",
    "        \"\"\"Compute the update for the Bernoulli parameters in the M-step of the E-M algorithm.\n",
    "        You should derive the optimal value of theta (the one which maximizes the expected log\n",
    "        probability) by setting the partial derivatives to zero. You should implement this in\n",
    "        terms of NumPy matrix and vector operations, rather than a for loop.\"\"\"\n",
    "\n",
    "        ######################## Your code here #########################\n",
    "        pass\n",
    "\n",
    "\n",
    "        #################################################################\n",
    "\n",
    "    def compute_posterior(self, X, M=None):\n",
    "        \"\"\"Compute the posterior probabilities of the cluster assignments given the observations.\n",
    "        This is used to compute the E-step of the E-M algorithm. It's also used in computing the\n",
    "        posterior predictive distribution when making inferences about the hidden part of the image.\n",
    "        It takes an optional parameter M, which is a binary matrix the same size as X, and determines\n",
    "        which pixels are observed. (1 means observed, and 0 means unobserved.)\n",
    "\n",
    "        Your job is to compute the variable log_p_z_x, which is a matrix whose (i, k) entry is the\n",
    "        log of the joint proability, i.e.\n",
    "             log p(z^(i) = k, x^(i)) = log p(z^(i) = k) + log p(x^(i) | z^(i) = k)\n",
    "\n",
    "        Hint: the solution is a small modification of the computation of log_p_z_x in\n",
    "        Model.log_likelihood.\n",
    "        \"\"\"\n",
    "        \n",
    "        if M is None:\n",
    "            M = np.ones(X.shape, dtype=int)\n",
    "\n",
    "        ######################## Your code here #########################\n",
    "        \n",
    "\n",
    "\n",
    "        #################################################################\n",
    "            \n",
    "        # subtract the max of each row to avoid numerical instability\n",
    "        log_p_z_x_shifted = log_p_z_x - log_p_z_x.max(1).reshape((-1, 1))\n",
    "\n",
    "        # convert the log probabilities to probabilities and renormalize\n",
    "        R = np.exp(log_p_z_x_shifted)\n",
    "        R /= R.sum(1).reshape((-1, 1))\n",
    "        return R\n",
    "\n",
    "    def posterior_predictive_means(self, X, M):\n",
    "        \"\"\"Compute the matrix of posterior predictive means for unobserved pixels given the observed\n",
    "        pixels. The matrix M is a binary matrix the same size as X which determines which pixels\n",
    "        are observed. (1 means observed, and 0 means unobserved.) You should return a real-valued\n",
    "        matrix the same size as X. For all the entries corresponding to unobserved pixels, the value\n",
    "        should determine the posterior probability that the pixel is on, conditioned on the observed\n",
    "        pixels. It does not matter what values you assign for observed pixels, since those values\n",
    "        aren't used for anything. Hint: the solution involves two very short lines, one of which is\n",
    "        a call to self.compute_posterior.\"\"\"\n",
    "\n",
    "        ######################## Your code here #########################\n",
    "        pass\n",
    "\n",
    "\n",
    "        #################################################################\n",
    "        \n",
    "    def visualize_components(self, title=None):\n",
    "        \"\"\"Visualize the learned components. Each of the images shows the Bernoulli parameters\n",
    "        (probability of the pixel being 1) for one of the mixture components.\"\"\"\n",
    "\n",
    "        pylab.figure('Mixture components')\n",
    "        pylab.matshow(util.arrange(self.params.theta.reshape((-1, IMAGE_DIM, IMAGE_DIM))),\n",
    "                      fignum=False, cmap='gray')\n",
    "        if title is None:\n",
    "            title = 'Mixture components'\n",
    "        pylab.title(title)\n",
    "        pylab.draw()\n",
    "\n",
    "    def visualize_predictions(self, X, M, title=None):\n",
    "        \"\"\"Visualize the predicted probabilities for each of the missing pixels.\"\"\"\n",
    "\n",
    "        P = self.posterior_predictive_means(X, M)\n",
    "        imgs = np.where(M, X, P)\n",
    "        obs = np.where(M, X, 0.3)\n",
    "\n",
    "        pylab.figure('Observations')\n",
    "        pylab.matshow(util.arrange(obs.reshape((-1, IMAGE_DIM, IMAGE_DIM))),\n",
    "                      fignum=False, cmap='gray')\n",
    "        pylab.title('Observations')\n",
    "\n",
    "        pylab.figure('Model predictions')\n",
    "        pylab.matshow(util.arrange(imgs.reshape((-1, IMAGE_DIM, IMAGE_DIM))),\n",
    "                      fignum=False, cmap='gray')\n",
    "        if title is None:\n",
    "            title = 'Model predictions'\n",
    "        pylab.title(title)\n",
    "        pylab.draw()\n",
    "        \n",
    "\n",
    "def train_from_labels(prior=None, show=True):\n",
    "    \"\"\"Fit the mixture model using the labeled MNIST data. There are 10 mixture components,\n",
    "    one corresponding to each of the digit classes.\"\"\"\n",
    "    \n",
    "    X = util.read_mnist_images(TRAIN_IMAGES_FILE)\n",
    "    y = util.read_mnist_labels(TRAIN_LABELS_FILE)\n",
    "    X_test = util.read_mnist_images(TEST_IMAGES_FILE)\n",
    "    num_data, num_pixels = X.shape\n",
    "\n",
    "    if prior is None:\n",
    "        prior = Prior.default_prior()\n",
    "    model = Model.random_initialization(prior, 10, IMAGE_DIM**2)\n",
    "\n",
    "    R = np.zeros((num_data, 10))\n",
    "    R[np.arange(num_data), y] = 1.\n",
    "    model.params.pi = model.update_pi(R)\n",
    "    model.params.theta = model.update_theta(X, R)\n",
    "\n",
    "    # mask which includes top half of pixels\n",
    "    M = np.zeros(X.shape, dtype=int)\n",
    "    M[:, :M.shape[1]//2] = 1\n",
    "\n",
    "    if show:\n",
    "        model.visualize_components()\n",
    "        try:\n",
    "            model.visualize_predictions(X[:64, :], M[:64, :])\n",
    "        except:\n",
    "            print('Posterior predictive distribution not implemented yet.')\n",
    "\n",
    "        print('Training log-likelihood:', model.log_likelihood(X) / num_data)\n",
    "        print('Test log-likelihood:', model.log_likelihood(X_test) / X_test.shape[0])\n",
    "\n",
    "    return model\n",
    "    \n",
    "        \n",
    "def train_with_em(num_components=100, num_steps=50, prior=None, draw_every=1):\n",
    "    \"\"\"Fit the mixture model in an unsupervised fashion using E-M.\"\"\"\n",
    "    \n",
    "    X = util.read_mnist_images(TRAIN_IMAGES_FILE)\n",
    "    X_test = util.read_mnist_images(TEST_IMAGES_FILE)\n",
    "    num_data, num_pixels = X.shape\n",
    "\n",
    "    if prior is None:\n",
    "        prior = Prior.default_prior()\n",
    "    model = Model.random_initialization(prior, num_components, num_pixels)\n",
    "\n",
    "    # mask which includes top half of pixels\n",
    "    M = np.zeros(X.shape, dtype=int)\n",
    "    M[:, :M.shape[1]//2] = 1\n",
    "\n",
    "    loglik_vals = []\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        # E-step\n",
    "        R = model.compute_posterior(X)\n",
    "\n",
    "        # M-step\n",
    "        model.params.pi = model.update_pi(R)\n",
    "        model.params.theta = model.update_theta(X, R)\n",
    "\n",
    "        loglik = model.log_likelihood(X) / num_data\n",
    "        loglik_vals.append(loglik)\n",
    "\n",
    "        if (i+1) % draw_every == 0:\n",
    "            model.visualize_components()\n",
    "            model.visualize_predictions(X[:64, :], M[:64, :])\n",
    "\n",
    "            pylab.figure('Log-likelihood')\n",
    "            pylab.clf()\n",
    "            pylab.semilogx(np.arange(1, i+2), loglik_vals)\n",
    "            pylab.title('Log-likelihood')\n",
    "            pylab.xlabel('Number of E-M steps')\n",
    "            pylab.draw()\n",
    "\n",
    "\n",
    "    print('Final training log-likelihood:', model.log_likelihood(X) / num_data)\n",
    "    print('Final test log-likelihood:', model.log_likelihood(X_test) / X_test.shape[0])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_log_probs_by_digit_class(model):\n",
    "    \"\"\"Print the average log-probability of images in each digit class.\"\"\"\n",
    "    \n",
    "    X = util.read_mnist_images(TRAIN_IMAGES_FILE)\n",
    "    y = util.read_mnist_labels(TRAIN_LABELS_FILE)\n",
    "    X_test = util.read_mnist_images(TEST_IMAGES_FILE)\n",
    "    y_test = util.read_mnist_labels(TEST_LABELS_FILE)\n",
    "\n",
    "    print('Training set')\n",
    "    for digit in range(10):\n",
    "        X_curr = X[y==digit, :]\n",
    "        loglik = model.log_likelihood(X_curr) / X_curr.shape[0]\n",
    "        print('Average log-probability of a {} image: {:1.3f}'.format(digit, loglik))\n",
    "    print()\n",
    "\n",
    "    print('Test set')\n",
    "    for digit in range(10):\n",
    "        X_curr = X_test[y_test==digit, :]\n",
    "        loglik = model.log_likelihood(X_curr) / X_curr.shape[0]\n",
    "        print('Average log-probability of a {} image: {:1.3f}'.format(digit, loglik))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_part_1_values():\n",
    "    \"\"\"Print a set of values that we use to check the correctness of the implementation in Part 1.\"\"\"\n",
    "\n",
    "    NUM_IMAGES = 50\n",
    "\n",
    "    X = util.read_mnist_images(TRAIN_IMAGES_FILE)\n",
    "    y = util.read_mnist_labels(TRAIN_LABELS_FILE)\n",
    "    X, y = X[:NUM_IMAGES, :], y[:NUM_IMAGES]\n",
    "    num_data, num_pixels = X.shape\n",
    "\n",
    "    prior = Prior(2., 3., 4.)\n",
    "    model = Model.random_initialization(prior, 10, IMAGE_DIM**2)\n",
    "\n",
    "    R = np.zeros((num_data, 10))\n",
    "    R[np.arange(num_data), y] = 0.9\n",
    "    R += 0.01\n",
    "    model.params.pi = model.update_pi(R)\n",
    "    model.params.theta = model.update_theta(X, R)\n",
    "\n",
    "    print('pi[0]', model.params.pi[0])\n",
    "    print('pi[1]', model.params.pi[1])\n",
    "    print('theta[0, 239]', model.params.theta[0, 239])\n",
    "    print('theta[3, 298]', model.params.theta[3, 298])\n",
    "\n",
    "\n",
    "\n",
    "def print_part_2_values():\n",
    "    \"\"\"Print a set of values that we use to check the correctness of the implementation in Part 2.\"\"\"\n",
    "    model = train_from_labels(show=False)\n",
    "\n",
    "    X = util.read_mnist_images(TRAIN_IMAGES_FILE)\n",
    "    \n",
    "    M = np.zeros(X.shape, dtype=int)\n",
    "    M[:, ::50] = 1\n",
    "\n",
    "    R = model.compute_posterior(X, M)\n",
    "    P = model.posterior_predictive_means(X, M)\n",
    "\n",
    "    print('R[0, 2]', R[0, 2])\n",
    "    print('R[1, 0]', R[1, 0])\n",
    "    print('P[0, 183]', P[0, 183])\n",
    "    print('P[2, 628]', P[2, 628])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
